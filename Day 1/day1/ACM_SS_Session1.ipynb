{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static vs Contextual Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static embeddings represent each word as fixed vector.\n",
    "\n",
    "Contextual embeddings leverage the context to represent each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-24 10:37:06.453437: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-24 10:37:07.452127: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-24 10:37:14.883819: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.2/lib64/stubs:/usr/local/cuda-12.2/lib64:/usr/local/cuda-12.2/lib64/stubs:/usr/local/cuda-12.2/lib64:\n",
      "2024-06-24 10:37:14.883908: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.2/lib64/stubs:/usr/local/cuda-12.2/lib64:/usr/local/cuda-12.2/lib64/stubs:/usr/local/cuda-12.2/lib64:\n",
      "2024-06-24 10:37:14.883920: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\u001b[33mDEPRECATION: https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0-py3-none-any.whl#egg=en_core_web_lg==3.0.0 contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting en-core-web-lg==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0-py3-none-any.whl (778.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.8/778.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:06\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from en-core-web-lg==3.0.0) (3.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.5 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (8.0.13)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.7.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.9.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.4.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.6)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.6.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.23.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.24.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.11.2)\n",
      "Requirement already satisfied: setuptools in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (50.3.1.post20201107)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/singh_shruti/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/singh_shruti/.local/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2020.6.20)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/singh_shruti/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.1.1)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"I went to the bank yesterday to withdraw cash.\"\n",
    "\n",
    "s2 = \"We went fishing by the river bank.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(s1)\n",
    "doc2 = nlp(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc1:\n",
    "    print(dir(token))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc1:\n",
    "    print(token.text, token.pos_)\n",
    "    if token.text == 'bank':\n",
    "        v1 = token.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(v1), v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc2:\n",
    "    print(token.text, token.pos_)\n",
    "    if token.text == 'bank':\n",
    "        v2 = token.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(v2), v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(v1 == v2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word2vec representation for the word bank in both the sentences is the same even though the sense of usage is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Embeddings using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 10:41:53.959254: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-24 10:41:54.100390: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-24 10:41:54.798918: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.2/lib64/stubs:/usr/local/cuda-12.2/lib64:/usr/local/cuda-12.2/lib64/stubs:/usr/local/cuda-12.2/lib64:\n",
      "2024-06-24 10:41:54.799027: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.2/lib64/stubs:/usr/local/cuda-12.2/lib64:/usr/local/cuda-12.2/lib64/stubs:/usr/local/cuda-12.2/lib64:\n",
      "2024-06-24 10:41:54.799033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac3e8e1250343489a50eb469d8e4e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_using_bert(text):\n",
    "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "    return last_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get vectors using BERT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_embed = \"We would like to embed this text!\"\n",
    "\n",
    "print(\"Vocab ids:\")\n",
    "print(tokenizer.encode(to_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Token ids:\")\n",
    "print(tokenizer.convert_ids_to_tokens([101, 2057, 2052, 2066, 2000, 7861, 8270, 2023, 3793, 999, 102]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = embed_text_using_bert(to_embed)\n",
    "print(vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing contextual vectors in 2d embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"I went to the bank yesterday to withdraw cash.\"\n",
    "s2 = \"We went fishing by the river bank.\"\n",
    "s3 = \"He has borrowed a loan from the bank.\"\n",
    "\n",
    "s4 = \"The bank issued a new 10 rupee note yesterday.\"\n",
    "s5 = \"Please note down the steps directed by the bank.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in [s1, s2, s3, s4, s5]:\n",
    "    print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens([2924])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1vec = embed_text_using_bert(s1)[0:4:]\n",
    "s1vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bank_contextual_vector(text):\n",
    "    bank_idx = tokenizer.encode(text).index(2924)\n",
    "    v1 = embed_text_using_bert(text).detach().numpy()[0, bank_idx]\n",
    "    print(v1.shape)\n",
    "    return v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1vec = get_bank_contextual_vector(s1)\n",
    "s2vec = get_bank_contextual_vector(s2)\n",
    "s3vec = get_bank_contextual_vector(s3)\n",
    "s4vec = get_bank_contextual_vector(s4)\n",
    "s5vec = get_bank_contextual_vector(s5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(s3vec == s2vec).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(np.array([s1vec, s2vec, s3vec, s4vec, s5vec]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.scatter(X_2d[:,0], X_2d[:, 1])\n",
    "ax.margins(0.17)\n",
    "\n",
    "for _, txt in enumerate(['bank cash', 'river bank', 'bank loan', 'bank note_curr', 'note_steps bank']):\n",
    "    ax.annotate(txt, (X_2d[_, 0]+0.2, X_2d[_, 1]), fontsize='x-small')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
