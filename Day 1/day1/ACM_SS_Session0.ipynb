{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2nTxc-Q9Lxqy",
        "outputId": "f07b6d79-a983-41b3-cb4d-33c0a9799dc2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def my_css():\n",
        "   display(HTML(\"\"\"\"\"\"))\n",
        "\n",
        "get_ipython().events.register('pre_run_cell', my_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VSESDHloMVZo",
        "outputId": "993af668-4011-4325-fc66-301b2b12f14f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.10/dist-packages (from folium==0.2.1) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2->folium==0.2.1) (2.1.5)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79794 sha256=3a0f658fa9957fe6f181a767080c0e61cd250b89f32c1634ed556f3fe2a5085d\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/0c/07/d7792a5444d5bb074361ac27da53cee9d5cce59a07fe9da5dd\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.14.0\n",
            "    Uninstalling folium-0.14.0:\n",
            "      Successfully uninstalled folium-0.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.17.6 requires folium>=0.9.1, but you have folium 0.2.1 which is incompatible.\n",
            "geemap 0.32.1 requires folium>=0.13.0, but you have folium 0.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed folium-0.2.1\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "geemap 0.32.1 requires folium>=0.13.0, but you have folium 0.2.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "cd16a4c3273b4e87bb1609f8eacdb133",
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install folium==0.2.1\n",
        "!pip install datasets\n",
        "\n",
        "# Make sure that we have a recent version of pyarrow in the session before we continue - otherwise reboot Colab to activate it\n",
        "import pyarrow\n",
        "if int(pyarrow.__version__.split('.')[1]) < 16 and int(pyarrow.__version__.split('.')[0]) == 0:\n",
        "    import os\n",
        "    os.kill(os.getpid(), 9)\n",
        "\n",
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K5jWQ2nkMX8Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wElMp85EMY9k"
      },
      "source": [
        "# [Language Processing Pipeline with Spacy](https://spacy.io/usage/processing-pipelines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5x-C7wgMYns"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9iqM1cRMnpR"
      },
      "outputs": [],
      "source": [
        "text = \"It was the best of times, it was the worst of times. It was the age of wisdom, it was the age of foolishness. It was the epoch of belief, it was the epoch of incredulity,. It was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair. We had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way—in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever.\"\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-ACvXrSMtTi"
      },
      "source": [
        "## Sentence Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-SvAHSJMq7N"
      },
      "outputs": [],
      "source": [
        "doc = nlp(text)\n",
        "for sent in doc.sents:\n",
        "    print(\">\", sent, sent.start, sent.end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DwzlEfwMwem"
      },
      "source": [
        "## Extracting entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBG0D57TMyDD"
      },
      "outputs": [],
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent, ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAHIMkWiM2qM"
      },
      "outputs": [],
      "source": [
        "apple_doc = nlp(\"Steve Jobs and Steve Wozniak incorporated Apple Computer on January 3, 1977, in Cupertino, California.\")\n",
        "\n",
        "for ent in apple_doc.ents:\n",
        "    print(ent.text, ent.label_)\n",
        "\n",
        "displacy.render(apple_doc, style=\"ent\", jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSYfjUy-M4zl"
      },
      "source": [
        "## Lemmatization, POS-Tags, Syntax Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ui4Y-l0nM5-D"
      },
      "outputs": [],
      "source": [
        "for sent in doc.sents:\n",
        "    for tok in sent:\n",
        "        print(tok, tok.lemma_, spacy.explain(tok.pos_), tok.is_stop)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UTowPhnM9cF"
      },
      "outputs": [],
      "source": [
        "for sent in doc.sents:\n",
        "    for tok in sent:\n",
        "        print(tok, tok.morph)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWyJtcgEM_Xd"
      },
      "outputs": [],
      "source": [
        "displacy.render(sent, style='dep', jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzN0TCUaNDUD"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUmHVqxJNE7N"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, TweetTokenizer, MWETokenizer\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfD-B7x8NG7N"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsBuWYULNKHZ"
      },
      "outputs": [],
      "source": [
        "text = \"I ate 8.5 ice-creams in New Delhi 🥶😇\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KoWimB0NLwx"
      },
      "outputs": [],
      "source": [
        "word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIlcV8L1NNuT"
      },
      "outputs": [],
      "source": [
        "tokenizer = TweetTokenizer()\n",
        "tokenizer.tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMGsKtruNPow"
      },
      "outputs": [],
      "source": [
        "tokenizer = MWETokenizer()\n",
        "tokenizer.add_mwe(('New', 'Delhi'))\n",
        "tokenizer.tokenize(word_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmKpS11KNR7G"
      },
      "source": [
        "Subword Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOq6bKLuNUCJ"
      },
      "source": [
        "**1. Byte-Pair Encoding (BPE):** BPE relies on a pre-tokenizer that splits the training data into words. Pretokenization can be as simple as space tokenization. After pre-tokenization, a set of unique words has been created and the frequency of each word it occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Used by GPT, GPT-2, RoBERTa models.  \n",
        "\n",
        "**2. WordPiece:** WordPiece first initializes the vocabulary to include every character present in the training data and progressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent symbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary. Used by BERT, DistilBERT, and Electra.  \n",
        "\n",
        "**3. SentencePiece:** Above tokenizers assumed that the input text uses spaces to separate words. However, not all languages use spaces to separate words. To solve this, SentencePiece treats the input as a raw input stream, thus including the space in the set of characters to use. It then uses the BPE algorithm to construct the appropriate vocabulary. Some models that use SP are ALBERT, XLNet, Marian, and T5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2rDoDYENS-q"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, BertTokenizer, XLNetTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErLbrOD_N34q"
      },
      "outputs": [],
      "source": [
        "gpt2_tokenizer   = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "bert_tokenizer   = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "xlnet_tokenizer  = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJTcSYMAN6Az"
      },
      "outputs": [],
      "source": [
        "text = \"It was the best of times, it was the worst of times.\"\n",
        "\n",
        "print(\"GPT2 Tokenizer: \", gpt2_tokenizer.tokenize(text))\n",
        "print(\"BERT Tokenizer: \", bert_tokenizer.tokenize(text))\n",
        "print(\"XLNT Tokenizer: \", xlnet_tokenizer.tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LjrTVKXN8UO"
      },
      "outputs": [],
      "source": [
        "to_embed = \"We would like to embed this extremely short text with an unknown word zozofah!\"\n",
        "\n",
        "print(gpt2_tokenizer.convert_ids_to_tokens(gpt2_tokenizer.encode(to_embed)))\n",
        "print(bert_tokenizer.convert_ids_to_tokens(bert_tokenizer.encode(to_embed)))\n",
        "print(xlnet_tokenizer.convert_ids_to_tokens(xlnet_tokenizer.encode(to_embed)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33WUJqZgN-n3"
      },
      "source": [
        "## Finetuning and Evaluation on MRPC dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkGpBrjYOP2w"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from datasets import load_dataset, list_datasets, list_metrics\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq9AdMMdOSit"
      },
      "outputs": [],
      "source": [
        "datasets = list_datasets()\n",
        "print(len(datasets), datasets[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhBggKJtOUTO"
      },
      "outputs": [],
      "source": [
        "glue_dataset = list_datasets(with_details=True)[datasets.index('glue')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPNU0O_2OVu0"
      },
      "outputs": [],
      "source": [
        "pprint(glue_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbjyQjFCOYF4"
      },
      "source": [
        "### GLUE: A MULTI-TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTANDING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJsPIwsSOamR"
      },
      "source": [
        "GLUE contains 11 tasks including MRPC, STS, QQP, and several NLI tasks. More details are available on https://gluebenchmark.com/tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNvE07EYOf2x"
      },
      "source": [
        "MRPC (Microsoft Research Paraphrase Corpus): https://www.microsoft.com/en-us/download/details.aspx?id=52398\n",
        "\n",
        "5800 pairs of sentences have been extracted from news sources on the web, along with human annotations indicating whether each pair captures a paraphrase/semantic equivalence relationship."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rva5lbBDOYgt"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset('glue', 'mrpc', split='train')\n",
        "test_dataset = load_dataset('glue', 'mrpc', split='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNFSxD2GO08r"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFnFbrv8O28m"
      },
      "outputs": [],
      "source": [
        "def encode(examples):\n",
        "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length')\n",
        "\n",
        "train_dataset = train_dataset.map(encode, batched=True)\n",
        "test_dataset = test_dataset.map(encode, batched=True)\n",
        "print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9HorXKQO402"
      },
      "outputs": [],
      "source": [
        "print(train_dataset[0].keys(), train_dataset[0]['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCqNyyB6O75F"
      },
      "source": [
        "Let's use a BERT model for [classification](https://huggingface.co/docs/transformers/en/model_doc/bert#transformers.BertForSequenceClassification)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFHgjcDPPNc8"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfoICLt0PRRb"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDmcnuyvPVa0"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hFmSsXCPW5I"
      },
      "outputs": [],
      "source": [
        "model.train().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqKi3WZVPhVs"
      },
      "outputs": [],
      "source": [
        "ckpt_path = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGE_hh2dPZMj"
      },
      "outputs": [],
      "source": [
        "for epoch in range(2):\n",
        "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if i % 10 == 0:\n",
        "            print(f\"loss: {loss}\")\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        torch.save(model, f'{ckpt_path}/model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-OP9PvQPlRX"
      },
      "outputs": [],
      "source": [
        "torch.save(model, f'{ckpt_path}/model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFK9imQdPruQ"
      },
      "outputs": [],
      "source": [
        "test_dataset = test_dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
        "test_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjG14cHwPuIN"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "all_ground_truth = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(tqdm(test_dataloader)):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        logits = model(**batch)[1]\n",
        "        predicted_class_ids = logits.argmax(dim=-1)\n",
        "        all_ground_truth += batch['labels'].cpu().detach().numpy().tolist()\n",
        "        all_predictions += predicted_class_ids.cpu().detach().numpy().tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFYSZGiOPwJk"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3_NYH_rPx1d"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qu6zUsxaPzrI"
      },
      "outputs": [],
      "source": [
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvWuF92XP1NS"
      },
      "outputs": [],
      "source": [
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "results = accuracy_metric.compute(references=all_ground_truth, predictions=all_predictions)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vfqMiwtP1y7"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG-TGfsIP4Up"
      },
      "source": [
        "1. Spacy - https://spacy.io/  \n",
        "2. Tokenization: https://neptune.ai/blog/tokenization-in-nlp  \n",
        "3. HF tokenizers: https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/tokenizer_summary.ipynb  \n",
        "4. GLUE https://openreview.net/pdf?id=rJ4km2R5t7  \n",
        "5. https://huggingface.co/docs/datasets/quickstart  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYnkkKHhP2-X"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
